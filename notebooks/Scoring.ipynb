{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b47fc2ae-876c-4434-8733-345ed812d677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2b36b0d-2d9b-47a1-a084-ca5208831371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,\"/home/vs428/Documents/DischargeMe/hail-dischargeme/scoring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6dcd2ae4-e1bb-466a-977f-9fc35beaa996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scoring import calculate_scores\n",
    "# from scoring import compute_overall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c0fd3921-948b-4e49-a669-219c14d176d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import evaluate\n",
    "\n",
    "from bleu import Bleu\n",
    "from rouge import Rouge\n",
    "from bertscore import BertScore  #\n",
    "\n",
    "\n",
    "def calculate_scores_bhc(generated, reference, metrics):\n",
    "    if not metrics:\n",
    "        raise ValueError(\"No metrics specified for scoring.\")\n",
    "    print(\"Beginning scoring...\")\n",
    "\n",
    "    scores = {}\n",
    "    for metric in metrics:\n",
    "        scores[metric] = {\"brief_hospital_course\": []}\n",
    "\n",
    "    # initialize scorers\n",
    "    if \"bleu\" in metrics:\n",
    "        bleuScorer = Bleu()\n",
    "        print(\"bleuScorer initialized\")\n",
    "    if \"rouge\" in metrics:\n",
    "        scores[\"rouge\"][\"brief_hospital_course\"] = [[], [], []]\n",
    "        rougeScorer = Rouge([\"rouge1\", \"rouge2\", \"rougeL\"])\n",
    "        print(\"rougeScorer initialized\")\n",
    "    if \"bertscore\" in metrics:\n",
    "        bertScorer = BertScore()\n",
    "        print(\"bertScorer initialized\")\n",
    "    if \"meteor\" in metrics:\n",
    "        meteorScorer = evaluate.load(\"meteor\")\n",
    "        print(\"meteorScorer initialized\")\n",
    "\n",
    "    def calculate_scores(rows_ref, rows_gen):\n",
    "        if \"bleu\" in metrics:\n",
    "            temp = bleuScorer(\n",
    "                refs=rows_ref[\"brief_hospital_course\"].tolist(),\n",
    "                hyps=rows_gen[\"brief_hospital_course\"].tolist(),\n",
    "            )\n",
    "            scores[\"bleu\"][\"brief_hospital_course\"].extend(temp)\n",
    "        if \"rouge\" in metrics:\n",
    "            temp = rougeScorer(\n",
    "                refs=rows_ref[\"brief_hospital_course\"].tolist(),\n",
    "                hyps=rows_gen[\"brief_hospital_course\"].tolist(),\n",
    "            )\n",
    "            scores[\"rouge\"][\"brief_hospital_course\"][0].extend(\n",
    "                    temp['rouge1'],\n",
    "            )\n",
    "            scores[\"rouge\"][\"brief_hospital_course\"][1].extend(\n",
    "                    temp['rouge2'],\n",
    "            )\n",
    "            scores[\"rouge\"][\"brief_hospital_course\"][2].extend(\n",
    "                    temp['rougeL'],\n",
    "            )\n",
    "        if \"bertscore\" in metrics:\n",
    "            temp = bertScorer(\n",
    "                refs=rows_ref[\"brief_hospital_course\"].tolist(),\n",
    "                hyps=rows_gen[\"brief_hospital_course\"].tolist(),\n",
    "            )\n",
    "            scores[\"bertscore\"][\"brief_hospital_course\"].extend(temp)\n",
    "        if \"meteor\" in metrics:\n",
    "            temp = meteorScorer.compute(\n",
    "                references=rows_ref[\"brief_hospital_course\"].tolist(),\n",
    "                predictions=rows_gen[\"brief_hospital_course\"].tolist(),\n",
    "            )\n",
    "            scores[\"meteor\"][\"brief_hospital_course\"].append(temp[\"meteor\"])\n",
    "\n",
    "        # print progress\n",
    "        current_row = i + 128\n",
    "        if current_row % 128 == 0:\n",
    "            print(f\"Processed {current_row}/{len(generated)} samples.\", flush=True)\n",
    "\n",
    "    reference.set_index(\"hadm_id\", drop=False, inplace=True)\n",
    "    generated.set_index(\"hadm_id\", drop=False, inplace=True)\n",
    "\n",
    "    batch_size = 128\n",
    "    for i in range(0, len(generated), batch_size):\n",
    "        rows_ref = reference[i : i + batch_size]\n",
    "        rows_gen = generated[i : i + batch_size]\n",
    "        calculate_scores(rows_ref=rows_ref, rows_gen=rows_gen)\n",
    "\n",
    "    print(f\"Processed {len(generated)}/{len(generated)} samples.\", flush=True)\n",
    "    print(\"Done.\")\n",
    "    return scores\n",
    "\n",
    "\n",
    "\n",
    "def compute_overall_score_bhc(scores):\n",
    "    print(\"Computing overall score...\")\n",
    "    leaderboard = {}\n",
    "\n",
    "    metrics = list(scores.keys())\n",
    "\n",
    "    if \"bleu\" in metrics:\n",
    "        bleu_brief_hospital_course = np.mean(scores[\"bleu\"][\"brief_hospital_course\"])\n",
    "        leaderboard[\"bleu\"] = np.mean(\n",
    "            [bleu_brief_hospital_course]\n",
    "        )\n",
    "    if \"rouge\" in metrics:\n",
    "        rouge_1_brief_hospital_course = np.mean(\n",
    "            scores[\"rouge\"][\"brief_hospital_course\"][0]\n",
    "        )\n",
    "        rouge_2_brief_hospital_course = np.mean(\n",
    "            scores[\"rouge\"][\"brief_hospital_course\"][1]\n",
    "        )\n",
    "        rouge_l_brief_hospital_course = np.mean(\n",
    "            scores[\"rouge\"][\"brief_hospital_course\"][2]\n",
    "        )\n",
    "\n",
    "        leaderboard[\"rouge1\"] = np.mean(\n",
    "            [rouge_1_brief_hospital_course]\n",
    "        )\n",
    "        leaderboard[\"rouge2\"] = np.mean(\n",
    "            [rouge_2_brief_hospital_course]\n",
    "        )\n",
    "        leaderboard[\"rougel\"] = np.mean(\n",
    "            [rouge_l_brief_hospital_course]\n",
    "        )\n",
    "    if \"bertscore\" in metrics:\n",
    "        bertscore_brief_hospital_course = np.mean(\n",
    "            scores[\"bertscore\"][\"brief_hospital_course\"]\n",
    "        )\n",
    "        leaderboard[\"bertscore\"] = np.mean(\n",
    "            [bertscore_brief_hospital_course]\n",
    "        )\n",
    "    if \"meteor\" in metrics:\n",
    "        meteor_brief_hospital_course = np.mean(\n",
    "            scores[\"meteor\"][\"brief_hospital_course\"]\n",
    "        )\n",
    "        leaderboard[\"meteor\"] = np.mean(\n",
    "            [meteor_brief_hospital_course]\n",
    "        )\n",
    "\n",
    "    # normalize sacrebleu to be between 0 and 1\n",
    "    for key in leaderboard.keys():\n",
    "        if key == \"sacrebleu\":\n",
    "            leaderboard[key] = leaderboard[key] / 100\n",
    "\n",
    "    overall_score = np.mean(list(leaderboard.values()))\n",
    "    leaderboard[\"overall\"] = overall_score\n",
    "\n",
    "    print(\"Done.\")\n",
    "    return leaderboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "993bf363-16db-46ba-9193-e5498e86aa77",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "calculate_scores_bhc() missing 3 required positional arguments: 'generated', 'reference', and 'metrics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcalculate_scores_bhc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: calculate_scores_bhc() missing 3 required positional arguments: 'generated', 'reference', and 'metrics'"
     ]
    }
   ],
   "source": [
    "calculate_scores_bhc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81fc848-c15d-4216-a9c1-2c49b6595d2c",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c649900-eae8-4d17-9a68-37e0abf3672a",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = pd.read_csv(\"/home/vs428/Documents/DischargeMe/hail-dischargeme/notebooks/brief_hospital_course/comparison_simple_gpt3.5_2shot_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "947c241c-0f64-4f92-a9d6-a081f05b5f59",
   "metadata": {
    "papermill": {
     "duration": 0.010561,
     "end_time": "2024-04-07T17:16:44.847682",
     "exception": false,
     "start_time": "2024-04-07T17:16:44.837121",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "challenge_data_fp = \"/gpfs/gibbs/project/rtaylor/shared/DischargeMe/public/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29d5cc66-620c-4cb6-b498-76ddeabb0519",
   "metadata": {},
   "outputs": [],
   "source": [
    "discharge_test = pd.read_csv(challenge_data_fp + \"test_phase_1/discharge.csv.gz\", keep_default_na=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f8989db-00fe-449f-be01-0d6a9102be54",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = notes.merge(discharge_test[['note_id',\"text\"]], on=\"note_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1601532-6dbb-4db6-8441-7a7cd11fb78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hadm_id,brief_hospital_course,discharge_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "705d472a-36de-4cc8-9e30-3cf2a25079da",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = notes[['hadm_id', \"output\"]].rename({\"output\":\"brief_hospital_course\"}, axis=1)\n",
    "generated = notes[['hadm_id', \"gpt_completion\"]].rename({\"gpt_completion\":\"brief_hospital_course\"}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4f4da6a-76e1-4d1c-9316-c98f850ea8f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning scoring...\n",
      "bleuScorer initialized\n",
      "rougeScorer initialized\n",
      "bertScorer initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/vs428/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/vs428/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/vs428/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meteorScorer initialized\n",
      "Processed 128/100 samples.\n",
      "Processed 100/100 samples.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "scores = calculate_scores_bhc(generated,\n",
    "                 reference,\n",
    "                 [\"bleu\", \"rouge\", \"bertscore\", \"meteor\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e1d6bddf-21a3-4ebe-9cc1-063428bc5953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing overall score...\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.021180955741754777,\n",
       " 'rouge1': 0.30068857756205297,\n",
       " 'rouge2': 0.08561870020674237,\n",
       " 'rougel': 0.15457243955124483,\n",
       " 'bertscore': 0.3117234826087952,\n",
       " 'meteor': 0.19592233654164518,\n",
       " 'overall': 0.17828441536870587}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_overall_score_bhc(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c4b43e-1f5c-4b13-bf3c-9a610c6559fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
