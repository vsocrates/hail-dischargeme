{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "version = \"v1\"\n",
    "\n",
    "inputcol = \"bhc_preceding_text\"\n",
    "outputcol = \"brief_hospital_course\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8VR8kd4ipPN"
   },
   "source": [
    "# Fine-tuning BART for DischargeMe Brief Hospital Course Task\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUn2OqI9oPQb"
   },
   "source": [
    "## Setup\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rimUDCQGoTAJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import datasets\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "from tabulate import tabulate\n",
    "import nltk\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'prompt_functions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprompt_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_pt_prompt_per_service\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'prompt_functions'"
     ]
    }
   ],
   "source": [
    "from prompt_functions import create_pt_prompt_per_service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'preprocessing'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data_injection\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'preprocessing'"
     ]
    }
   ],
   "source": [
    "from preprocessing import data_injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8zpflBQbzrvC"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvimig-socrates\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "WANDB_INTEGRATION = True\n",
    "if WANDB_INTEGRATION:\n",
    "    import wandb\n",
    "\n",
    "    wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynvml import *\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\", flush=True)\n",
    "\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\", flush=True)\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\", flush=True)\n",
    "    print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aX-q_O-hoe3g"
   },
   "source": [
    "## Model and tokenizer\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yb21WY-9mavn"
   },
   "source": [
    "Download model and tokenizer. Use default parameters or try custom values (see [HF Bart configuration](https://huggingface.co/transformers/_modules/transformers/configuration_bart.html) and [Fairseq Bart](https://github.com/pytorch/fairseq/tree/master/examples/bart))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320,
     "referenced_widgets": [
      "177dd46c6d5444aca5f500a9c9a8360c",
      "d8eb6df708df47bcaa680c45389f9789",
      "905192b1adad4ee298f82af962f67592",
      "d7834d5eecf9412391c7492f37e8e3ee",
      "ccbbe4c9daf043398c7029c926666275",
      "e9bf4b882c2041c584a4b0eb22e7fa70",
      "99f0d622b7e4430b85eccbfd8ddb00b3",
      "b4e2344137864b8ebd6ad78437c3a19a",
      "7cf993132faa4d89a64fd3eca312cdbc",
      "fbf907eddfe24e9399f10ea6d89f8f7d",
      "759aaa8b51b04df4b0287e0f76d9c20b",
      "6b1a1421e6e64fb6906f3e9a6aed5f21",
      "7efd4ea0ea5347399ed0b621dde70079",
      "965d92fa10254517b62c352d64dbde4d",
      "02ce7e45725d4ea8bb9e92e510ce461d",
      "4be0831bc19149919ce1d88a15740088",
      "6bce83a3cef143b6a111acdf64da0c4e",
      "b95c2cdea4bc40b08a05713e533e3aa9",
      "ba3fda5f01a1472884f3f9d0b0fc108d",
      "eaca0e42cbe04191be4cfc9fb50c1e1c",
      "32f7cae84a444897a2b4c0392bd9116f",
      "60d92abae8cd45fc9c33e62d441419a9",
      "7a0f47ee32c747388c52d9d18b43bf1c",
      "67f9afc232a04ffca994891d8aed9912",
      "a83ef7bd2dd440b99a5e9e0b509cfa3d",
      "ff705e5b53874c4085ce820e2e45fbef",
      "59c23e3de2554a72b405f947ea4dbdb3",
      "14aaadb3a50e46d0b4f05e6d0750696c",
      "70ccbe5adf3f48b1941ec90daa146989",
      "4ab611bbbc9c41588f02fbc3d879d6b9",
      "48a6682d09984cffa3a608a22dec4305",
      "d56a6f544c4942d48cd912b39d739619",
      "9033abd90902440db83b521af2362607",
      "620f66af1b874233b13b07b987bf5d5c",
      "de232831629e4de19886353c1c7dd639",
      "9b91df4680264521a9ce63306a5914fe",
      "2eab5b3f9aed410892d083b09d6da827",
      "a51df7c4e4bc43579173d14c203b71a8",
      "cbc1850d082e4d9fbb05c5b07026e0e2",
      "45d6fd520cec4b6ab4f385efea2393d1",
      "1ed29c61bb7240aebbd0fbda10a8fd1c",
      "be8f53768dcd466e9716a5fa36eb0b64",
      "fb58dc38229148fa9937dde37c923e24",
      "c6e527cc8c144b4db881b469f34be746",
      "061238de11e44920857de4521e98a51b",
      "67227a5963184d3ca75fa0aafb8ca072",
      "77b798c81e154102a71ac0a519bbe8a9",
      "8dcdd0dc6063416eb90800ea3046070c"
     ]
    },
    "editable": true,
    "id": "7vMhyyIPobyx",
    "outputId": "e28badd9-654b-475b-9ac0-a12b35ccdc2e",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"GanjinZero/biobart-large\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set model parameters or use the default\n",
    "# print(model.config)\n",
    "\n",
    "# tokenization\n",
    "encoder_max_length = 1024  # demo\n",
    "decoder_max_length = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wwtSPRJgomBS"
   },
   "source": [
    "## Data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OZfrIK8fW9DU"
   },
   "source": [
    "### Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kV9bhaJw1dbA"
   },
   "source": [
    "For demonstration, we are only using a small portion of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 93 MB.\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_pickle(\"/gpfs/gibbs/project/rtaylor/shared/DischargeMe/public/train/discharge_target_with_preceding_text+structured_data.pickle\")\n",
    "valid_data = pd.read_pickle(\"/gpfs/gibbs/project/rtaylor/shared/DischargeMe/public/valid/discharge_target_with_preceding_text+structured_data.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    68785.000000\n",
       "mean       327.462397\n",
       "std        236.807308\n",
       "min         10.000000\n",
       "25%        163.000000\n",
       "50%        281.000000\n",
       "75%        440.000000\n",
       "max       3435.000000\n",
       "Name: brief_hospital_course_word_count, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['brief_hospital_course_word_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset.from_pandas(train_data[['hadm_id', \"bhc_preceding_text\", \"brief_hospital_course\"]], split=\"train\")\n",
    "valid_ds = Dataset.from_pandas(valid_data[['hadm_id', \"bhc_preceding_text\", \"brief_hospital_course\"]], split=\"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = datasets.load_dataset(\"wiki_lingua\", name=language, split=\"train[:200]\")\n",
    "\n",
    "# Take a look at the data\n",
    "# print(train_ds[0])\n",
    "\n",
    "# data['article'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sjGvlbT6XBe6"
   },
   "source": [
    "### Prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BD7uaok9474K"
   },
   "source": [
    "**Format and split into train and validation sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118,
     "referenced_widgets": [
      "8b43a79aaaf44bb4ae497911ec1cb069",
      "fff5dd9c027d4e0ca53102bb59289d64",
      "b28730387e29480fa43743dfbc6ba7a5",
      "d5ab7ba210954b4d8fd5e175ab78c636",
      "4fa1015ca37848088ab1c66488edfd20",
      "642a892a1999461ab4a35975b23fe330",
      "df9c9948711a4bb6b5c6864095fc9a9f",
      "e284a90cb37345eea8f2ecdd5ef65940",
      "db16d22c18c440b0be96b99a2b144819",
      "6d0028934af84743b9de85ed7172b931",
      "82e9ca2b98fc40c3a07bbfa62055b197",
      "bd3a31515c02447e9e5574d420e781a8",
      "b5c0b01cb2114d5c93f800997b663f87",
      "a5fa1ead5ecb4d688969746b489c741d",
      "c91957e3842c4af7baf3403b4554b2bc",
      "4dc306fd47e44b5ca5e14da342362eba"
     ]
    },
    "id": "M3TeTKzQ49CO",
    "outputId": "fdcc5de9-a4da-4baf-c4f1-fe93b42c77b9"
   },
   "outputs": [],
   "source": [
    "def flatten(example):\n",
    "    return {\n",
    "        \"document\": example[\"bhc_preceding_text\"],\n",
    "        \"summary\": example[\"brief_hospital_course\"],\n",
    "    }\n",
    "\n",
    "\n",
    "def list2samples(example):\n",
    "    documents = []\n",
    "    summaries = []\n",
    "    for sample in zip(example[\"document\"], example[\"summary\"]):\n",
    "        if len(sample[0]) > 0:\n",
    "            documents += sample[0]\n",
    "            summaries += sample[1]\n",
    "    return {\"document\": documents, \"summary\": summaries}\n",
    "\n",
    "\n",
    "train_dataset_txt = train_ds.map(flatten, remove_columns=['hadm_id', \"bhc_preceding_text\", \"brief_hospital_course\"])\n",
    "# we don't need to do this step because we don't have multiple doc/summary pairs within each example\n",
    "# train_dataset = train_dataset.map(list2samples, batched=True)\n",
    "\n",
    "valid_dataset_txt = valid_ds.map(flatten, remove_columns=['hadm_id', \"bhc_preceding_text\", \"brief_hospital_course\"])\n",
    "# valid_dataset = valid_dataset.map(list2samples, batched=True)\n",
    "\n",
    "# train_data_txt, validation_data_txt = dataset.train_test_split(test_size=0.1).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 68785\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pbe750YpMfD"
   },
   "source": [
    "**Preprocess and tokenize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118,
     "referenced_widgets": [
      "d746734a9c51418c864801d55a09638b",
      "ce0b55032013487b912bb982971f126d",
      "3cb71c39c7d04451b89acdec2efbe964",
      "c98e43401f4b43e49d6625cefc9467de",
      "1e2df782a99b4273a80334f1d44780ac",
      "e2cceac1ede3433f887a716ad177a160",
      "0eef45d80219425e982d0ea0a78344f9",
      "c979275e29b544d0b8e982a049bf8e22",
      "75678a6b33d54fec8500e96728966970",
      "5ed5ea35b4bc4d10afee05becdc467d7",
      "f35b5e590d6a4fff9c1005c99201ffbf",
      "7b6c91323d4f4c1e9e0647429940fd53",
      "54513cf682ce408c88f856803e390bea",
      "a9e45262444746138b274573415c171d",
      "e490b67d80884bdbbaaf97e2ce5fb0ad",
      "9021f9227b8948bba7dfeabba65c9e55"
     ]
    },
    "editable": true,
    "id": "PyksYNwxA4OM",
    "outputId": "dc317686-0868-4fe5-d7ee-bbb84de6836f",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We already did this, so we can just load this dataset\n",
    "\n",
    "# def batch_tokenize_preprocess(batch, tokenizer, max_source_length, max_target_length):\n",
    "#     source, target = batch[\"document\"], batch[\"summary\"]\n",
    "#     source_tokenized = tokenizer(\n",
    "#         source, padding=\"max_length\", truncation=True, max_length=max_source_length\n",
    "#     )\n",
    "#     target_tokenized = tokenizer(\n",
    "#         target, padding=\"max_length\", truncation=True, max_length=max_target_length\n",
    "#     )\n",
    "\n",
    "#     batch = {k: v for k, v in source_tokenized.items()}\n",
    "#     # Ignore padding in the loss\n",
    "#     batch[\"labels\"] = [\n",
    "#         [-100 if token == tokenizer.pad_token_id else token for token in l]\n",
    "#         for l in target_tokenized[\"input_ids\"]\n",
    "#     ]\n",
    "#     return batch\n",
    "\n",
    "\n",
    "# train_data = train_dataset_txt.map(\n",
    "#     lambda batch: batch_tokenize_preprocess(\n",
    "#         batch, tokenizer, encoder_max_length, decoder_max_length\n",
    "#     ),\n",
    "#     batched=True,\n",
    "#     remove_columns=train_dataset_txt.column_names,\n",
    "# )\n",
    "\n",
    "# validation_data = valid_dataset_txt.map(\n",
    "#     lambda batch: batch_tokenize_preprocess(\n",
    "#         batch, tokenizer, encoder_max_length, decoder_max_length\n",
    "#     ),\n",
    "#     batched=True,\n",
    "#     remove_columns=valid_dataset_txt.column_names,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data.save_to_disk(\"/gpfs/gibbs/project/rtaylor/shared/DischargeMe/public/train/discharge_target_with_preceding_text_BioBART_data.hf\")\n",
    "# validation_data.save_to_disk(\"/gpfs/gibbs/project/rtaylor/shared/DischargeMe/public/valid/discharge_target_with_preceding_text_BioBART_data.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "train_data = load_from_disk(\"/gpfs/gibbs/project/rtaylor/shared/DischargeMe/public/train/discharge_target_with_preceding_text_BioBART_data.hf\")\n",
    "validation_data = load_from_disk(\"/gpfs/gibbs/project/rtaylor/shared/DischargeMe/public/valid/discharge_target_with_preceding_text_BioBART_data.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After processing datasets\n",
      "GPU memory occupied: 93 MB.\n"
     ]
    }
   ],
   "source": [
    "print(\"After processing datasets\", flush=True)\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(test_samples, model):\n",
    "    inputs = tokenizer(\n",
    "        test_samples[\"document\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=encoder_max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    return inputs\n",
    "    input_ids = inputs.input_ids.to(model.device)\n",
    "    attention_mask = inputs.attention_mask.to(model.device)\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask, num_beams=4, min_length=200, max_length=1500, early_stopping=True)\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return outputs, output_str\n",
    "\n",
    "\n",
    "model_before_tuning = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = generate_summary(train_dataset_txt[:2], model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,  1437, 50118,  ...,   288,  3226,     2],\n",
       "        [    0,  1437, 50118,  ...,  6617,   256,     2]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data\n",
    "\n",
    "input_ids = inputs.input_ids.to(model.device)\n",
    "attention_mask = inputs.attention_mask.to(model.device)\n",
    "outputs = model.generate(input_ids, attention_mask=attention_mask, num_beams=4, min_length=200, max_length=1500, early_stopping=True)\n",
    "# output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,  1437, 50249,  ...,     1,     1,     1],\n",
       "        [    2,  1437, 50249,  ...,   384,  1640,     2]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "h7ViBmMopWfb",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Training\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EfTztMPv2vG"
   },
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68,
     "referenced_widgets": [
      "1eb628ce400d48d39e77b1950e928025",
      "fcd19732a34c47b5b2ca5a643f2d085d",
      "6086a4b108b74f219fff01890d22cd20",
      "1f743d0d7b154cafb344ed0c015f6910",
      "48397443928d452285fa439aaa6f369a",
      "4c8584515ed14f92b06136d6bb21629d",
      "b444d0d09e1b46b3a6382e90aa9c9512",
      "eeb32fd422934ecb8406f4b49e924de8"
     ]
    },
    "editable": true,
    "id": "rpNCGl2sYl2p",
    "outputId": "228587e5-ffbc-4bf8-b9d2-6f63e3d50420",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3674939/4242988070.py:5: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = datasets.load_metric(\"rouge\")\n",
      "/home/vs428/.conda/envs/peft_finetune_env/lib/python3.12/site-packages/datasets/load.py:756: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/rouge/rouge.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Borrowed from https://github.com/huggingface/transformers/blob/master/examples/seq2seq/run_summarization.py\n",
    "\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "metric = datasets.load_metric(\"rouge\")\n",
    "\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "    # Extract a few results from ROUGE\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "\n",
    "    prediction_lens = [\n",
    "        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds\n",
    "    ]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8O1EeUi-pbPA"
   },
   "source": [
    "### Training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 2\n",
    "# train_size = train_data.num_rows\n",
    "# train_batch_size = 8\n",
    "# ga_steps = 1\n",
    "# virtual_batch_size = train_batch_size * ga_steps   # \"invented name\" => 256\n",
    "# per_epoch_steps = int(train_size / virtual_batch_size + 0.5) # round => 121\n",
    "# total_steps = epochs * per_epoch_steps # => 605"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true,
    "id": "6R9d7ELIpX9F",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vs428/.conda/envs/peft_finetune_env/lib/python3.12/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Using auto half precision backend\n"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"bart-dischargeme-results_{version}\",\n",
    "    num_train_epochs=2,  # demo\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=8,  # demo\n",
    "    per_device_eval_batch_size=2,\n",
    "    # learning_rate=3e-05,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.1,\n",
    "    label_smoothing_factor=0.1,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir=f\"bart-dischargeme-logs_{version}\",\n",
    "    logging_steps=5,\n",
    "    save_total_limit=2,\n",
    "\n",
    "    save_strategy=\"steps\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=250,\n",
    "    save_steps=500,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    load_best_model_at_end=True,\n",
    "    greater_is_better=False,\n",
    "    disable_tqdm=True,\n",
    "    log_level=\"info\",\n",
    "    logging_first_step=True,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=validation_data.shuffle(seed=42).select(range(2000)),\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience = 5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== After setting up trainer and args ==============\n",
      "GPU memory occupied: 1791 MB.\n"
     ]
    }
   ],
   "source": [
    "print(\"============== After setting up trainer and args ==============\", flush=True)\n",
    "print_gpu_utilization()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qzcsz3gKplPO"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rpg2a0mfoD-l"
   },
   "source": [
    "Wandb integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true,
    "id": "tdaVPp9doF1c",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02bee703bd6a4f28a0b29c1d198594c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112348362803458, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/vast/palmer/home.mccleary/vs428/Documents/DischargeMe/hail-dischargeme/notebooks/brief_hospital_course/template_code/wandb/run-20240424_171819-ev4mgmg6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vimig-socrates/bart-dischargeme/runs/ev4mgmg6' target=\"_blank\">firm-waterfall-15</a></strong> to <a href='https://wandb.ai/vimig-socrates/bart-dischargeme' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vimig-socrates/bart-dischargeme' target=\"_blank\">https://wandb.ai/vimig-socrates/bart-dischargeme</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vimig-socrates/bart-dischargeme/runs/ev4mgmg6' target=\"_blank\">https://wandb.ai/vimig-socrates/bart-dischargeme/runs/ev4mgmg6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if WANDB_INTEGRATION:\n",
    "    wandb_run = wandb.init(\n",
    "        project=\"bart-dischargeme\",\n",
    "        config={\n",
    "            \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n",
    "            \"learning_rate\": training_args.learning_rate,\n",
    "            \"dataset\": \"dischargeme preceding_text_only\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H%M%S\")\n",
    "    wandb_run.name = \"run_\" + current_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kEtd_a7TPpkd"
   },
   "source": [
    "Evaluate before fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Before trainer.evaluate() pretrain ==============\n",
      "GPU memory occupied: 2086 MB.\n"
     ]
    }
   ],
   "source": [
    "print(\"============== Before trainer.evaluate() pretrain ==============\", flush=True)\n",
    "print_gpu_utilization()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "editable": true,
    "id": "5yveDiz7pm3i",
    "outputId": "e0793d3b-389c-40a4-ea4a-c0d4c15a8017",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== After trainer.evaluate() pretrain ==============\n",
      "GPU memory occupied: 2086 MB.\n"
     ]
    }
   ],
   "source": [
    "print(\"============== After trainer.evaluate() pretrain ==============\", flush=True)\n",
    "print_gpu_utilization()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "nkRb7hvgPrf2",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 847
    },
    "editable": true,
    "id": "qYcYcbkr7ZZD",
    "outputId": "84cf21c8-5a40-4f44-82ed-55a83c7d5100",
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51' max='17198' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   51/17198 00:32 < 3:09:28, 1.51 it/s, Epoch 0.01/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='156' max='7360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 156/7360 00:54 < 42:02, 2.86 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vs428/.conda/envs/peft_finetune_env/lib/python3.12/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#%%wandb\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# uncomment to display Wandb charts\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/peft_finetune_env/lib/python3.12/site-packages/transformers/trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1622\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/peft_finetune_env/lib/python3.12/site-packages/transformers/trainer.py:2029\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2026\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   2027\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2029\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2030\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2031\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/.conda/envs/peft_finetune_env/lib/python3.12/site-packages/transformers/trainer.py:2412\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2410\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 2412\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2413\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2415\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/peft_finetune_env/lib/python3.12/site-packages/transformers/trainer_seq2seq.py:166\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gen_kwargs \u001b[38;5;241m=\u001b[39m gen_kwargs\n\u001b[0;32m--> 166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/peft_finetune_env/lib/python3.12/site-packages/transformers/trainer.py:3229\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3226\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3228\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3229\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3230\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3232\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3233\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3237\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3239\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/.conda/envs/peft_finetune_env/lib/python3.12/site-packages/transformers/trainer.py:3418\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3415\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[1;32m   3417\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[0;32m-> 3418\u001b[0m loss, logits, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3419\u001b[0m main_input_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain_input_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3420\u001b[0m inputs_decode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs[main_input_name]) \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_inputs_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/peft_finetune_env/lib/python3.12/site-packages/transformers/trainer_seq2seq.py:296\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.prediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys, **gen_kwargs)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m generation_inputs\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m generation_inputs\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m generation_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m generation_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    292\u001b[0m ):\n\u001b[1;32m    293\u001b[0m     generation_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    294\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_attention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    295\u001b[0m     }\n\u001b[0;32m--> 296\u001b[0m generated_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgeneration_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;66;03m# Temporary hack to ensure the generation config is not initialized for each iteration of the evaluation loop\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;66;03m# TODO: remove this hack when the legacy code that initializes generation_config from a model config is\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;66;03m# removed in https://github.com/huggingface/transformers/blob/98d88b23f54e5a23e741833f1e973fdf600cc2c5/src/transformers/generation/utils.py#L1183\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39m_from_model_config:\n",
      "File \u001b[0;32m~/.conda/envs/peft_finetune_env/lib/python3.12/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/peft_finetune_env/lib/python3.12/site-packages/transformers/generation/utils.py:1609\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[1;32m   1593\u001b[0m         input_ids,\n\u001b[1;32m   1594\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1604\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1605\u001b[0m     )\n\u001b[1;32m   1607\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1608\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m-> 1609\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m \u001b[43mBeamSearchScorer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_beams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlength_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlength_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_early_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_beam_hyps_to_keep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_return_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1617\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1618\u001b[0m     \u001b[38;5;66;03m# 12. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   1619\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1620\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1621\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   1622\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1623\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1624\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/peft_finetune_env/lib/python3.12/site-packages/transformers/generation/beam_search.py:195\u001b[0m, in \u001b[0;36mBeamSearchScorer.__init__\u001b[0;34m(self, batch_size, num_beams, device, length_penalty, do_early_stopping, num_beam_hyps_to_keep, num_beam_groups, max_length)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_beam_hyps \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    185\u001b[0m     BeamHypotheses(\n\u001b[1;32m    186\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_beam_groups)\n\u001b[1;32m    192\u001b[0m ]\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# self._done[i*self.num_beam_groups+j] indicates whether the generation of the beam_hyps of the j-th group\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# in the i-th mini-batch is complete.\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_done \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_beam_groups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(num_beams, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m num_beams \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`num_beams` has to be an integer strictly greater than 1, but is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_beams\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. For `num_beams` == 1,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m one should make use of `greedy_search` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    203\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#%%wandb\n",
    "# uncomment to display Wandb charts\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"============== After trainer.train() ==============\", flush=True)\n",
    "print_gpu_utilization()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3C-4SfOPssY"
   },
   "source": [
    "Evaluate after fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "editable": true,
    "id": "_-QyUtCRH9DO",
    "outputId": "0b5b5583-0ba1-462d-9645-37422a3333e2",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "b752706b4845420f939aae77bad05b8d",
      "3725321613124a9fb555bd4ef31035fd",
      "dd16f78c32f2412498419687cde38779",
      "d0e495d4dddb488799142d6a2550872b",
      "1a31a81cb51541b089a0b1e6df281963",
      "131ff7a29b1a4853a3cd840f78904a07",
      "e626643516fa4c3f87d72ee7406107cd",
      "62c757374f504b54bd911d333b349e2e"
     ]
    },
    "editable": true,
    "id": "ClRTrG2ETUm3",
    "outputId": "98c2c66a-09ec-4889-85e3-41c661623426",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if WANDB_INTEGRATION:\n",
    "    wandb_run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-gSLVnGL9bol"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "hDwj24cfILS6",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Generate summaries from the fine-tuned model and compare them with those generated from the original, pre-trained one.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /home/vs428/Documents/DischargeMe/hail-dischargeme/notebooks/brief_hospital_course/template_code/bart-dischargeme-results_v2/checkpoint-16500/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"/home/vs428/Documents/DischargeMe/hail-dischargeme/notebooks/brief_hospital_course/template_code/bart-dischargeme-results_v2/checkpoint-16500/\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file /home/vs428/Documents/DischargeMe/hail-dischargeme/notebooks/brief_hospital_course/template_code/bart-dischargeme-results_v2/checkpoint-16500/model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at /home/vs428/Documents/DischargeMe/hail-dischargeme/notebooks/brief_hospital_course/template_code/bart-dischargeme-results_v2/checkpoint-16500/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "loading configuration file /home/vs428/Documents/DischargeMe/hail-dischargeme/notebooks/brief_hospital_course/template_code/bart-dischargeme-results_v2/checkpoint-16500/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"/home/vs428/Documents/DischargeMe/hail-dischargeme/notebooks/brief_hospital_course/template_code/bart-dischargeme-results_v2/checkpoint-16500/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true,
    "id": "NV64-XdA_rOM",
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_summary(test_samples, model):\n",
    "    inputs = tokenizer(\n",
    "        test_samples[\"document\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=encoder_max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = inputs.input_ids.to(model.device)\n",
    "    attention_mask = inputs.attention_mask.to(model.device)\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask, num_beams=4, min_length=200, max_length=1500, early_stopping=True)\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return outputs, output_str\n",
    "\n",
    "\n",
    "model_before_tuning = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 34s, sys: 653 ms, total: 3min 34s\n",
      "Wall time: 3min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_samples = valid_dataset_txt.select(range(3))\n",
    "x, summaries_before_tuning = generate_summary(test_samples, model_before_tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, summaries_after_tuning = generate_summary(test_samples, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D7IPtJLjCcmS",
    "outputId": "fa863bb8-8bb7-4988-f322-faab93b6d395"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Id  Summary Gold                                                       Summary BART\n",
      "----  -----------------------------------------------------------------  ----------------------------------------------------------------\n",
      "   0  Ms. ___ is a ___ year old right-handed female with a               Ms. ___ is a ___ year old lady with lupus anticoagulant,\n",
      "      significant history of lupus anticoagulant, recurrent PEs ___,     history of recurrent PE (___), and long-standing\n",
      "      ___, and longstanding anxiety/panic attacks who presented on       anxiety/panic attacks currently on coumadin who\n",
      "      ___ with acute onset chest pain and right eye blurry               presents today with 1.5 days of chest pain acutely\n",
      "      vision.                                                            worsening today accompanied by monocular blurry vision\n",
      "                                                                         out of the right eye.\n",
      "      # NEURO: She was admitted to Neurology for workup of the blurred\n",
      "      vision in her right eye which had developed in the ED. MRI brain   As per Ms. ___, she has been having intermittent chest\n",
      "      and orbits with/without contrast showed no evidence of optic       pain ___ times/week for the last month. Pain occurs\n",
      "      neuritis. Ophthalmology eveluated her twice with dilated           when she takes a shallow breath and then worsens when she\n",
      "      fundoscopy and found no evidence of vascular lesions. ESR/CRP      take a deep breath. It usually resolves in 5 minutes and\n",
      "      were normal.                                                        rated ___. However, 2 days ago she developed\n",
      "      Visual acuity varied throughout hospitalization, between ___       this usual pain but it took about 30min to\n",
      "      - ___ in the right eye over the course of hours and between        resolve. There was associated tenderness to palpation under her\n",
      "      ___ and ___ in the left eye over course of hours, with             left breast and she felt a lump as well. The following\n",
      "      inconsistencies (e.g. still able to count fingers in the right     day she again had the same event. Today, she had\n",
      "      eye despite acuity of ___. No evidence of keratitis or             no pain at all AM, and as per her PCP's recommendations\n",
      "      corneal abrasions. ESR/CRP unremarkable. Etiology of the vision    underwent a mammogram this afternoon. A few minutes\n",
      "      changes was ultimately unclear but there was concern for           after the mammogram, she developed intense chest pain\n",
      "      functional element.                                                that felt like a deep pain (she describes it as\n",
      "                                                                         intermittent like \"labor pains\"). This then became a\n",
      "      # CARDIOVASC: Initial presentation to ED was for chest pain         crushing, pressure like pain like \"someone\n",
      "      (developed vision changes while in the ED). Troponins negative x   sat on her chest.\" This now radiated to her right\n",
      "      3, with CTA chest showing no pulmonary emboli. EKG WNL. Of note    scapula and her back and rated ___. She\n",
      "      her INR was 1, which may have been due to significant weight       presented to the ER where her NIHSS=0 but she was\n",
      "      gain and pt eating large amounts of leafy greens. She will see     noted to have weakness of right hand grip.\n",
      "      her PCP ___ ___ to increase her Coumadin dose.\n",
      "                                                                         On exam, the patient endorses headache, and blurred\n",
      "      # ENDOCRINE: TSH elevated to 16 in setting of pt stopping her      vision from R eye as well as photophobia in that\n",
      "      Levothyroxine several months ago (just restarted two days PTA).    eye. Denies any diplopia, or any vision changes out\n",
      "      She will continue Levothyroxine 200mg daily on discharge. Needs    of the left eye. She endorses photophobia\n",
      "      recheck TSH in ___ weeks.                                          in theright eye, right periorbital pressure, as well\n",
      "                                                                         as a migraine like headache in her left\n",
      "      # PSYCH: Started Sertraline 25mg daily for significant anxiety     forehead. As per her, her most recent INR was 2.6.\n",
      "      and panic attacks. Also gave small prescription for LZP 0.25mg\n",
      "      PRN anxiety. She will follow up with her PCP and is strongly\n",
      "      encouraged to pursue further mental health care.\n",
      "\n",
      "      =======================\n",
      "      TRANSITIONS OF CARE:\n",
      "      -- Needs recheck TSH in ___ weeks\n",
      "      -- Will need Coumadin increased by PCP (he has been emailed)\n",
      "      -- Has f/u with PCP ___ on ___ at\n",
      "      9:10 am.\n",
      "   1  Mr. ___ ___ yo M with history of CHB s/p pacemaker ___,            Mr ___ is a ___ year old man with a history of\n",
      "      HLD, HTN, OSA on CPAP who presents from ___ after a                complete heart block s/p pacemaker, hyperlipidemia,\n",
      "      recent admission for respiratory failure due to CHF and severe     hypertension, mitral valve prolapse s/P MVR\n",
      "      MR ___ repaired with CSURG ___ with cough and                      ___, OSA, possible SIADH, recently admitted\n",
      "      bilateral pleuritic chest pain.  Admission CXR consistent with     for respiratory failure due to CHF and severe MR, discharged to\n",
      "      pneumonia so patient treated with HAP coverage vanc/ceftazidime    rehab, presenting with one day of pleuritic bilateral\n",
      "      with significant improvement.  Sputum samples did not ultimately   chest pain and dyspnea.\n",
      "      provide an organism.  De-escalated abx and returns to rehab\n",
      "      ___.                                                               # Dyspnea:\n",
      "                                                                         # Pleuritic chest pain:\n",
      "                                                                         Patient presented with pleural effusion and\n",
      "                                                                         dyspnea on exertion. CXR did not show any evidence\n",
      "                                                                         of pneumonia. Patient was diuresed with IV lasix\n",
      "                                                                         and transitioned to PO torsemide prior to discharge.\n",
      "                                                                         He was continued on his home metoprolol succinate\n",
      "                                                                         12.5mg daily. He was also started on a bowel regimen\n",
      "                                                                         to encourage bowel movement. He will follow up with\n",
      "                                                                         pulmonology as an outpatient.\n",
      "   2  ___ is an ___ yo woman with medical history of DM and HTN          ___ is a pleasant ___ yo woman with\n",
      "      was transferred from an OSH for management of a new thalamic       medical history of HTN who was transferred from an\n",
      "      IPH.                                                               OSH for management of a new thalamic IPH. Per\n",
      "                                                                         report she was in her usual state of health today,\n",
      "      # Thalamic Intraparenchymal Hemorrhage:                            and developed mild confusion. So her family took her to\n",
      "      She presented to the ED after developing mild confusion and gait   ___. There a NCHCT showed intraparenchymal\n",
      "      unsteadiness. Head CT showed intraparenchymal hemorrhage. She      hemorrhage. At the time she was noted to have an SBP of\n",
      "      was started on nicardipine gtt for SBP of 200. Her initial         200, so she was started on nicardipine gtt.\n",
      "      neurologic exam was remarkable for mild inattention, as well as\n",
      "      mild right sided weakness. Etiology of the IPH is likely HTN       # NEURO: Patient was admitted to the ICU for close\n",
      "      given the location and hypertension at presentation. The           monitoring. She was started empirically on Keppra\n",
      "      differential diagnosis also includes vascular abnormality (like    for seizure prophylaxis. Repeat CT head on ___\n",
      "      AVM or cavernoma), underlying tumor, amyloid, or underlying        showed stable hemorrhage. Patient was transferred to the\n",
      "      stroke but all of those are much less likely. She was admitted     floor on ___. She was continued on her home\n",
      "      to the ICU for close blood pressure control with a nicardipine     antihypertensives. Her blood pressure goal was\n",
      "      drip to maintain systolic blood pressure under 140.                liberalized to SBP <140. She will follow up with her\n",
      "                                                                         PCP for further management.\n",
      "      Patient was transitioned to the floor and her home medications\n",
      "      were restarted and uptitrated as needed to maintain her blood      # CV: Patient's blood pressure was liberalized\n",
      "      pressure goals. She was evaluated by ___ who thought she would     to SBP<140. Her home atenolol was continued.\n",
      "      benefit from rehab given her cognitive impairment. Her exam was\n",
      "      normal except for impairment in memory and mild impairment in\n",
      "      gait when distracted prior to discharge.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    tabulate(\n",
    "        zip(\n",
    "            range(len(summaries_after_tuning)),\n",
    "            test_samples[\"summary\"],\n",
    "            summaries_after_tuning,\n",
    "        ),\n",
    "        headers=[\"Id\", \"Summary Gold\", \"Summary BART\"],\n",
    "    )\n",
    ")\n",
    "# print(\"\\nTarget summaries:\\n\")\n",
    "# print(\n",
    "#     tabulate(list(enumerate(test_samples[\"summary\"])), headers=[\"Id\", \"Target summary\"])\n",
    "# )\n",
    "# print(\"\\nSource documents:\\n\")\n",
    "# print(tabulate(list(enumerate(test_samples[\"document\"])), headers=[\"Id\", \"Document\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "fine_tune_bart_summarization_two_langs.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
