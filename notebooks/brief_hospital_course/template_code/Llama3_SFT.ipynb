{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "529098c5-18c4-4f0f-af40-5800b45a5e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45f4e77c-3328-4ee0-b193-552f809848b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimdb\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = load_dataset(\"imdb\", split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace94c5a-65ae-4690-9642-82a10a67555b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = SFTTrainer(\n",
    "    \"facebook/opt-350m\",\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92e5bc5f-d711-46c8-b4f9-17af010df7d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'SFTConfig' from 'trl' (/home/vs428/.conda/envs/peft_finetune_env/lib/python3.12/site-packages/trl/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 69\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrich\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     70\u001b[0m     ModelConfig,\n\u001b[1;32m     71\u001b[0m     RichProgressCallback,\n\u001b[1;32m     72\u001b[0m     SFTConfig,\n\u001b[1;32m     73\u001b[0m     SFTTrainer,\n\u001b[1;32m     74\u001b[0m     get_peft_config,\n\u001b[1;32m     75\u001b[0m     get_quantization_config,\n\u001b[1;32m     76\u001b[0m     get_kbit_device_map,\n\u001b[1;32m     77\u001b[0m )\n\u001b[1;32m     79\u001b[0m tqdm\u001b[38;5;241m.\u001b[39mpandas()\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'SFTConfig' from 'trl' (/home/vs428/.conda/envs/peft_finetune_env/lib/python3.12/site-packages/trl/__init__.py)"
     ]
    }
   ],
   "source": [
    "# flake8: noqa\n",
    "# Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"\n",
    "# regular:\n",
    "python examples/scripts/sft.py \\\n",
    "    --model_name_or_path=\"facebook/opt-350m\" \\\n",
    "    --report_to=\"wandb\" \\\n",
    "    --learning_rate=1.41e-5 \\\n",
    "    --per_device_train_batch_size=64 \\\n",
    "    --gradient_accumulation_steps=16 \\\n",
    "    --output_dir=\"sft_openassistant-guanaco\" \\\n",
    "    --logging_steps=1 \\\n",
    "    --num_train_epochs=3 \\\n",
    "    --max_steps=-1 \\\n",
    "    --push_to_hub \\\n",
    "    --gradient_checkpointing\n",
    "\n",
    "# peft:\n",
    "python examples/scripts/sft.py \\\n",
    "    --model_name_or_path=\"facebook/opt-350m\" \\\n",
    "    --report_to=\"wandb\" \\\n",
    "    --learning_rate=1.41e-5 \\\n",
    "    --per_device_train_batch_size=64 \\\n",
    "    --gradient_accumulation_steps=16 \\\n",
    "    --output_dir=\"sft_openassistant-guanaco\" \\\n",
    "    --logging_steps=1 \\\n",
    "    --num_train_epochs=3 \\\n",
    "    --max_steps=-1 \\\n",
    "    --push_to_hub \\\n",
    "    --gradient_checkpointing \\\n",
    "    --use_peft \\\n",
    "    --lora_r=64 \\\n",
    "    --lora_alpha=16\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import os\n",
    "from contextlib import nullcontext\n",
    "\n",
    "TRL_USE_RICH = os.environ.get(\"TRL_USE_RICH\", False)\n",
    "\n",
    "# from trl.commands.cli_utils import init_zero_verbose, SFTScriptArguments, TrlParser\n",
    "\n",
    "if TRL_USE_RICH:\n",
    "    init_zero_verbose()\n",
    "    FORMAT = \"%(message)s\"\n",
    "\n",
    "    from rich.console import Console\n",
    "    from rich.logging import RichHandler\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "from tqdm.rich import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from trl import (\n",
    "    ModelConfig,\n",
    "    RichProgressCallback,\n",
    "    SFTConfig,\n",
    "    SFTTrainer,\n",
    "    get_peft_config,\n",
    "    get_quantization_config,\n",
    "    get_kbit_device_map,\n",
    ")\n",
    "\n",
    "tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cb07b2-c4da-4256-9d40-dc3f021dcb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if TRL_USE_RICH:\n",
    "    logging.basicConfig(format=FORMAT, datefmt=\"[%X]\", handlers=[RichHandler()], level=logging.INFO)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = TrlParser((SFTScriptArguments, SFTConfig, ModelConfig))\n",
    "    args, training_args, model_config = parser.parse_args_and_config()\n",
    "\n",
    "    # Force use our print callback\n",
    "    if TRL_USE_RICH:\n",
    "        training_args.disable_tqdm = True\n",
    "        console = Console()\n",
    "\n",
    "    ################\n",
    "    # Model & Tokenizer\n",
    "    ################\n",
    "    torch_dtype = (\n",
    "        model_config.torch_dtype\n",
    "        if model_config.torch_dtype in [\"auto\", None]\n",
    "        else getattr(torch, model_config.torch_dtype)\n",
    "    )\n",
    "    quantization_config = get_quantization_config(model_config)\n",
    "    model_kwargs = dict(\n",
    "        revision=model_config.model_revision,\n",
    "        trust_remote_code=model_config.trust_remote_code,\n",
    "        attn_implementation=model_config.attn_implementation,\n",
    "        torch_dtype=torch_dtype,\n",
    "        use_cache=False if training_args.gradient_checkpointing else True,\n",
    "        device_map=get_kbit_device_map() if quantization_config is not None else None,\n",
    "        quantization_config=quantization_config,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_config.model_name_or_path, use_fast=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    ################\n",
    "    # Dataset\n",
    "    ################\n",
    "    raw_datasets = load_dataset(args.dataset_name)\n",
    "\n",
    "    train_dataset = raw_datasets[args.dataset_train_split]\n",
    "    eval_dataset = raw_datasets[args.dataset_test_split]\n",
    "\n",
    "    ################\n",
    "    # Optional rich context managers\n",
    "    ###############\n",
    "    init_context = nullcontext() if not TRL_USE_RICH else console.status(\"[bold green]Initializing the SFTTrainer...\")\n",
    "    save_context = (\n",
    "        nullcontext()\n",
    "        if not TRL_USE_RICH\n",
    "        else console.status(f\"[bold green]Training completed! Saving the model to {training_args.output_dir}\")\n",
    "    )\n",
    "\n",
    "    ################\n",
    "    # Training\n",
    "    ################\n",
    "    with init_context:\n",
    "        trainer = SFTTrainer(\n",
    "            model=model_config.model_name_or_path,\n",
    "            model_init_kwargs=model_kwargs,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            peft_config=get_peft_config(model_config),\n",
    "            callbacks=[RichProgressCallback] if TRL_USE_RICH else None,\n",
    "        )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    with save_context:\n",
    "        trainer.save_model(training_args.output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
